{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADEDrug_PP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ec7784798f147429135b8c426ad638a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3013983c09f442f3bcd331be28a658f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_43616cf406ef46f8a5464d4e18532097",
              "IPY_MODEL_c8dcf6430ee540feabb6667750937bee"
            ]
          }
        },
        "3013983c09f442f3bcd331be28a658f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43616cf406ef46f8a5464d4e18532097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_596e04efbf0c4e28815bd58261c50193",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9a066f1d6a24cd5a892ee77c0035699"
          }
        },
        "c8dcf6430ee540feabb6667750937bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d676aeb4f9a6409292f2f0d4133f5f26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:00&lt;00:00, 634B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41c95cdbab324aa185a1ebce21a7f4cb"
          }
        },
        "596e04efbf0c4e28815bd58261c50193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9a066f1d6a24cd5a892ee77c0035699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d676aeb4f9a6409292f2f0d4133f5f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41c95cdbab324aa185a1ebce21a7f4cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42e85555b09645e18f7038d42dde4718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea5a3498f5d64413be1b53254ef93b6e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_971911accaf540038cafafb2a9581bf4",
              "IPY_MODEL_c3a92db42613415b95de4062affd010d"
            ]
          }
        },
        "ea5a3498f5d64413be1b53254ef93b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "971911accaf540038cafafb2a9581bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70974ebca3634c16ab5bc58626fb0dc4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 227845,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 227845,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca7e68de2014481fbd6ce98ee4a0f347"
          }
        },
        "c3a92db42613415b95de4062affd010d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_acfa4dbb166a4ba583824420a7fea1e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 228k/228k [00:00&lt;00:00, 575kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb0a031d189a44cb98b409f0a4500606"
          }
        },
        "70974ebca3634c16ab5bc58626fb0dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca7e68de2014481fbd6ce98ee4a0f347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acfa4dbb166a4ba583824420a7fea1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb0a031d189a44cb98b409f0a4500606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0974933c0304a909b0ee12aca85b00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b61f8f2afcfe492480649e59c6fdb137",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87ffb4180c3b4bf395b9562356f66d46",
              "IPY_MODEL_c797c9399df443af986dc0c25831fb36"
            ]
          }
        },
        "b61f8f2afcfe492480649e59c6fdb137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87ffb4180c3b4bf395b9562356f66d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_60e6aa60114c4212a20e1bd8874c3941",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442221694,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442221694,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b7d6ab9afbf412d8c85d86e4c3db702"
          }
        },
        "c797c9399df443af986dc0c25831fb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0aebfdb598c42aabe7ffb0903099af4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442M/442M [00:11&lt;00:00, 38.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eabdf39ce49f45e2afd8720242991d70"
          }
        },
        "60e6aa60114c4212a20e1bd8874c3941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b7d6ab9afbf412d8c85d86e4c3db702": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0aebfdb598c42aabe7ffb0903099af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eabdf39ce49f45e2afd8720242991d70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQHjUXf3fI-F",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "8f325f5d-e729-48cb-8240-931ea2c694c4"
      },
      "source": [
        "from google.colab import files \n",
        "  \n",
        "  \n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd \n",
        "import io \n",
        "  \n",
        "df = pd.read_csv(io.BytesIO(uploaded['dfADE2PP.csv'])) \n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0297fc45-514c-4718-b99a-c68eb6c8e0fe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0297fc45-514c-4718-b99a-c68eb6c8e0fe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dfADE2PP.csv to dfADE2PP (1).csv\n",
            "                                                 corpus  ... ADE_Drug_label\n",
            "0      L1 compression fracture After the patient fel...  ...              1\n",
            "1      L1 compression fracture After the patient fel...  ...              0\n",
            "2      The patient had no localizing deficits on ser...  ...              1\n",
            "3      The patient had no localizing deficits on ser...  ...              0\n",
            "4      He was treated with pain medication including...  ...              1\n",
            "...                                                 ...  ...            ...\n",
            "4442   The patient is not a good candidate for aspir...  ...              1\n",
            "4443   The patient is not a good candidate for aspir...  ...              0\n",
            "4444   The patient was to be cardioverted but self q...  ...              1\n",
            "4445   <ADE> Bleeding <ADE> Postop due to anticoagul...  ...              1\n",
            "4446   <ADE> Bleeding <ADE> Postop due to anticoagul...  ...              0\n",
            "\n",
            "[4447 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DgcfPa1Lv8m",
        "outputId": "8d6b427e-8864-4c9a-ec7e-2555712413ce"
      },
      "source": [
        "df.sample(5)\n",
        "df['ADE_Drug_label'].sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OqUgCJxb8Kb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "f0bd5b53-606a-4d76-845b-d94fe54dc326"
      },
      "source": [
        "from google.colab import files \n",
        "  \n",
        "  \n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd \n",
        "import io \n",
        "df_testf = pd.read_csv(io.BytesIO(uploaded['dfADE_testFPP.csv'])) \n",
        "\n",
        "print(df_testf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ed2cd5a-fb88-4406-8f2f-fc642e18618b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ed2cd5a-fb88-4406-8f2f-fc642e18618b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dfADE_testFPP.csv to dfADE_testFPP.csv\n",
            "                                                 corpus  ... ADE_Drug_label\n",
            "0     Service Doctor Last Name 1181 MEDICINE HISTORY...  ...              1\n",
            "1     Service Doctor Last Name 1181 MEDICINE HISTORY...  ...              0\n",
            "2     Service Doctor Last Name 1181 MEDICINE HISTORY...  ...              1\n",
            "3     Service Doctor Last Name 1181 MEDICINE HISTORY...  ...              0\n",
            "4     Service Doctor Last Name 1181 MEDICINE HISTORY...  ...              1\n",
            "...                                                 ...  ...            ...\n",
            "4060  HR 130s EKG showed sinus tacchycardia Received...  ...              0\n",
            "4061  EKG showed sinus tacchycardia Received lovenox...  ...              0\n",
            "4062  Received lovenox for PE since he couldnt get C...  ...              0\n",
            "4063  EKG showed sinus tacchycardia Received lovenox...  ...              0\n",
            "4064  Received lovenox for PE since he couldnt get C...  ...              0\n",
            "\n",
            "[4065 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rms-cUOmmQJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9cda09-41b3-44ea-825c-9288191ed6f0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 26.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4c635c8ab37419fb5e5eefc720976a15ecd142ad7a90cbe4c5f67467e35fd645\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJVcSy4i9jP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4234fe3a-8eb8-4f64-e7d0-0d2ff5075cc9"
      },
      "source": [
        "\n",
        "import transformers\n",
        "import torch\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import rc\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid',palette = 'muted', font_scale = 1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 5,6\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9cTgtyZlOgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bd7f69-ae9a-45bf-bd45-be25c7104890"
      },
      "source": [
        "#@title Setup & Config\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpRKTBlLpofs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For cleaning the text\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import regex as re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2CPVCMjkQev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4a67cdbe-897c-40da-9a19-73b1adf6ab98"
      },
      "source": [
        "def to_rating(rating):\n",
        "  rating = int(rating)\n",
        "  if rating == 1:\n",
        "    return 'Positive Label'\n",
        "  else: \n",
        "    return 'Negative Label'\n",
        "\n",
        "df['ADE_Drug_Rating'] = df.ADE_Drug_label.apply(to_rating)\n",
        "df_testf['ADE_Drug_Rating'] = df.ADE_Drug_label.apply(to_rating)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>L1 compression fracture After the patient fel...</td>\n",
              "      <td>['leukocytosis', 'steroids']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L1 compression fracture After the patient fel...</td>\n",
              "      <td>['leukocytosis', 'prednisone']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The patient had no localizing deficits on ser...</td>\n",
              "      <td>['leukocytosis', 'steroids']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The patient had no localizing deficits on ser...</td>\n",
              "      <td>['leukocytosis', 'prednisone']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>He was treated with pain medication including...</td>\n",
              "      <td>['leukocytosis', 'steroids']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              corpus  ... ADE_Drug_Rating\n",
              "0   L1 compression fracture After the patient fel...  ...  Positive Label\n",
              "1   L1 compression fracture After the patient fel...  ...  Negative Label\n",
              "2   The patient had no localizing deficits on ser...  ...  Positive Label\n",
              "3   The patient had no localizing deficits on ser...  ...  Negative Label\n",
              "4   He was treated with pain medication including...  ...  Positive Label\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp4rAXttjKih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "bcdfccf6-cfc1-4cee-cb45-0734c008abe8"
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.countplot(df.ADE_Drug_Rating,palette=[\"#8F00FF\",\"#FF7D00\"])\n",
        "plt.xlabel('Data Balance: ADE-Drug');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 737,
              "height": 484
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZeHbgXXgaUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033e704e-2218-4c2e-a37d-3ac8a80aedfc"
      },
      "source": [
        "#----not run----\n",
        "nlp = spacy.load(\"en\")\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# spacy (362 words)\n",
        "spacy_st = nlp.Defaults.stop_words\n",
        "# nltk(179 words)\n",
        "nltk_st = stopwords.words('english')\n",
        "\n",
        "def clean(tweet, http = True, punc = True, lem = True, stop_w = True):\n",
        "    \n",
        "    if http is True:\n",
        "        tweet = re.sub(\"https?:\\/\\/t.co\\/[A-Za-z0-9]*\", '', tweet)\n",
        "\n",
        "    # stop words\n",
        "    # in here I changed the placement of lower for those of you who want to use\n",
        "    # Cased BERT later on.\n",
        "    if stop_w == 'nltk':\n",
        "        tweet = [word for word in word_tokenize(tweet) if not word.lower() in nltk_st]\n",
        "        tweet = ' '.join(tweet)\n",
        "\n",
        "    elif stop_w == 'spacy':\n",
        "        tweet = [word for word in word_tokenize(tweet) if not word.lower() in spacy_st]\n",
        "        tweet = ' '.join(tweet)\n",
        "\n",
        "    # lemmitizing\n",
        "    if lem == True:\n",
        "        lemmatized = [word.lemma_ for word in sp(tweet)]\n",
        "        tweet = ' '.join(lemmatized)\n",
        "\n",
        "    # punctuation removal\n",
        "    if punc is True:\n",
        "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "    # removing extra space\n",
        "    tweet = re.sub(\"\\s+\", ' ', tweet)\n",
        "    \n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UkmdkaEjJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "6ec7784798f147429135b8c426ad638a",
            "3013983c09f442f3bcd331be28a658f0",
            "43616cf406ef46f8a5464d4e18532097",
            "c8dcf6430ee540feabb6667750937bee",
            "596e04efbf0c4e28815bd58261c50193",
            "c9a066f1d6a24cd5a892ee77c0035699",
            "d676aeb4f9a6409292f2f0d4133f5f26",
            "41c95cdbab324aa185a1ebce21a7f4cb",
            "42e85555b09645e18f7038d42dde4718",
            "ea5a3498f5d64413be1b53254ef93b6e",
            "971911accaf540038cafafb2a9581bf4",
            "c3a92db42613415b95de4062affd010d",
            "70974ebca3634c16ab5bc58626fb0dc4",
            "ca7e68de2014481fbd6ce98ee4a0f347",
            "acfa4dbb166a4ba583824420a7fea1e2",
            "fb0a031d189a44cb98b409f0a4500606"
          ]
        },
        "outputId": "a3a9e823-88d9-4b9e-c11a-0d3c7f4aeded"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case = True)\n",
        "\n",
        "#SCBmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec7784798f147429135b8c426ad638a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42e85555b09645e18f7038d42dde4718",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Ly81PctUbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f6043ffc-8153-4bd0-91b1-27f926245c6e"
      },
      "source": [
        "'''class BERT_RE():\n",
        "    def __init__(self,bert=SCBmodel,dropout = 0.1):\n",
        "        super(BERT_RE, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()   \n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "        #self.to(self.device)\n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        prob = self.sigmoid(linear_output)\n",
        "        return prob\n",
        "\n",
        "    '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"class BERT_RE():\\n    def __init__(self,bert=SCBmodel,dropout = 0.1):\\n        super(BERT_RE, self).__init__()\\n        self.bert = bert\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear = nn.Linear(768, 1)\\n        self.sigmoid = nn.Sigmoid()   \\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\n        #self.to(self.device)\\n    def forward(self, tokens, masks=None):\\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\\n        dropout_output = self.dropout(pooled_output)\\n        linear_output = self.linear(dropout_output)\\n        prob = self.sigmoid(linear_output)\\n        return prob\\n\\n    \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGM691qwlBEO"
      },
      "source": [
        "MAX_LEN = 160\n",
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvoqrAkvjYm1"
      },
      "source": [
        "class BERT_RE(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(BERT_RE, self).__init__()\n",
        "    #self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.bert = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2LRvKHCSMnk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "b0974933c0304a909b0ee12aca85b00d",
            "b61f8f2afcfe492480649e59c6fdb137",
            "87ffb4180c3b4bf395b9562356f66d46",
            "c797c9399df443af986dc0c25831fb36",
            "60e6aa60114c4212a20e1bd8874c3941",
            "0b7d6ab9afbf412d8c85d86e4c3db702",
            "a0aebfdb598c42aabe7ffb0903099af4",
            "eabdf39ce49f45e2afd8720242991d70"
          ]
        },
        "outputId": "927188e7-b833-4ce0-9fc4-5d58eaa1b80a"
      },
      "source": [
        "model = BERT_RE(2)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0974933c0304a909b0ee12aca85b00d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442221694.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Y8RQLEoZiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1cb37f9e-94e2-45c8-acbe-f4462fa307eb"
      },
      "source": [
        "df.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>L1 compression fracture After the patient fel...</td>\n",
              "      <td>['leukocytosis', 'steroids']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L1 compression fracture After the patient fel...</td>\n",
              "      <td>['leukocytosis', 'prednisone']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The patient had no localizing deficits on ser...</td>\n",
              "      <td>['leukocytosis', 'steroids']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The patient had no localizing deficits on ser...</td>\n",
              "      <td>['leukocytosis', 'prednisone']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              corpus  ... ADE_Drug_Rating\n",
              "0   L1 compression fracture After the patient fel...  ...  Positive Label\n",
              "1   L1 compression fracture After the patient fel...  ...  Negative Label\n",
              "2   The patient had no localizing deficits on ser...  ...  Positive Label\n",
              "3   The patient had no localizing deficits on ser...  ...  Negative Label\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDLgdT4BoZTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppSnAf6nlaRo"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=(['[CLS]'] + df.corpus + ['[SEP]']).to_numpy(),\n",
        "    targets=df.ADE_Drug_label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTHfcDUMlmbj"
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrjlcrmywMf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "126a60b6-d3c1-40eb-af8a-e2297424647a"
      },
      "source": [
        "df_testf.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Service Doctor Last Name 1181 MEDICINE HISTORY...</td>\n",
              "      <td>['eye discharge', 'Dilantin']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Service Doctor Last Name 1181 MEDICINE HISTORY...</td>\n",
              "      <td>['eye discharge', 'sulfate']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Service Doctor Last Name 1181 MEDICINE HISTORY...</td>\n",
              "      <td>['oral sores', 'Dilantin']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Service Doctor Last Name 1181 MEDICINE HISTORY...</td>\n",
              "      <td>['oral sores', 'sulfate']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              corpus  ... ADE_Drug_Rating\n",
              "0  Service Doctor Last Name 1181 MEDICINE HISTORY...  ...  Positive Label\n",
              "1  Service Doctor Last Name 1181 MEDICINE HISTORY...  ...  Negative Label\n",
              "2  Service Doctor Last Name 1181 MEDICINE HISTORY...  ...  Positive Label\n",
              "3  Service Doctor Last Name 1181 MEDICINE HISTORY...  ...  Negative Label\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZKaWCcru5bs"
      },
      "source": [
        "df_test_set_val,_ = train_test_split(df_testf, test_size=0.05, random_state=RANDOM_SEED)        #test set that was uploaded later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlJ_mi56vDuH"
      },
      "source": [
        "df_test_set_val = df_test_set_val.append(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxEYIgN3uSix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "5802bfe8-1af0-4d47-9db1-09571b253f98"
      },
      "source": [
        "df_train.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Severe COPD Hypercarbic Respiratory Distress ...</td>\n",
              "      <td>['hypercarbia', 'benzodiazepine']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Cogwheel rigiditymasked faciesresting tremor O...</td>\n",
              "      <td>['extrapyrimidal symptoms', 'antipsychotics']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                corpus  ... ADE_Drug_Rating\n",
              "33    Severe COPD Hypercarbic Respiratory Distress ...  ...  Negative Label\n",
              "113  Cogwheel rigiditymasked faciesresting tremor O...  ...  Positive Label\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am7xSq1DuW_1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "c52cbbb3-8f06-4fa3-a25c-ee1610810f03"
      },
      "source": [
        "df_test.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "      <th>cleaned_corpus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>[' Hypotension: Appears to have been developin...</td>\n",
              "      <td>['Hypotension', 'simvastatin']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "      <td>Hypotension Appears developing subacutely ove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[' Pulmonary embolism/DVTs: She has had multip...</td>\n",
              "      <td>['rectus hematoma', 'lovenox']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "      <td>Pulmonary embolismDVTs multiple PEs hadnone e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                corpus  ...                                     cleaned_corpus\n",
              "259  [' Hypotension: Appears to have been developin...  ...   Hypotension Appears developing subacutely ove...\n",
              "39   [' Pulmonary embolism/DVTs: She has had multip...  ...   Pulmonary embolismDVTs multiple PEs hadnone e...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Ne1j-EucnW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0fa01ab5-dab7-499b-f296-cd76c8878d25"
      },
      "source": [
        "df_val.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "      <th>cleaned_corpus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>[' HTN: held ACE and BB for concern for possib...</td>\n",
              "      <td>['hypotension', 'ACE']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "      <td>HTN held ACE BB concern possible new bleeding...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>[\" FEN:  Speech and Swallow Recs - video swall...</td>\n",
              "      <td>['bled', 'Cinacalcet']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "      <td>FEN Speech Swallow Recs video swallow silentn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                corpus  ...                                     cleaned_corpus\n",
              "388  [' HTN: held ACE and BB for concern for possib...  ...   HTN held ACE BB concern possible new bleeding...\n",
              "281  [\" FEN:  Speech and Swallow Recs - video swall...  ...   FEN Speech Swallow Recs video swallow silentn...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HueFSHCJlg14"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loaderf = create_data_loader(df_test_set_val, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pex0_7fgkFYh"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9wLgrTekJ-A"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnUNZ9z6kNuy"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrjSnhhikR_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fd5d8d-3ece-4430-8be3-657b4be274a8"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "  val_accf, val_lossf = eval_model(\n",
        "    model,\n",
        "    test_data_loaderf,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_testf)\n",
        "  )\n",
        "\n",
        "  print(f'Test   loss final {val_lossf} accuracy final {val_accf}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  history['test_accF'].append(val_accf)\n",
        "  history['test_lossF'].append(val_lossf)\n",
        "  \n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7003658377079375 accuracy 0.5632183908045977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.545143859727042 accuracy 0.7072072072072072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.5507276763847052 accuracy final 0.670110701107011\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.60459743066138 accuracy 0.6714142928535732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6935406093086515 accuracy 0.6936936936936937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.6207038775579004 accuracy final 0.6784747847478474\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.5473423449997883 accuracy 0.7128935532233883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.45485604873725344 accuracy 0.7612612612612613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.47619605125967135 accuracy final 0.727921279212792\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.49597288804700174 accuracy 0.7881059470264867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.4877237709505217 accuracy 0.8153153153153153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.5083034371048951 accuracy final 0.7507995079950799\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.4274844987340182 accuracy 0.81984007996002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.4517012472663607 accuracy 0.8288288288288288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.5405599602367267 accuracy final 0.7483394833948339\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3748493283630842 accuracy 0.853823088455772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5503309518098831 accuracy 0.7972972972972973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.6241706015526756 accuracy final 0.7439114391143911\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.36865582095971144 accuracy 0.8640679660169914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.449005340891225 accuracy 0.8378378378378378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.6306475412316066 accuracy final 0.7407134071340713\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33113584560583315 accuracy 0.8830584707646176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.3859637938439846 accuracy 0.8558558558558559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.5759399286971605 accuracy final 0.753751537515375\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.29272482504110886 accuracy 0.9012993503248375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.45507347158023287 accuracy 0.8423423423423423\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.7292670907437309 accuracy final 0.7249692496924969\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.27422185813585126 accuracy 0.9060469765117442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.38672592996486593 accuracy 0.8603603603603603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test   loss final 0.6543687157609985 accuracy final 0.7375153751537514\n",
            "\n",
            "CPU times: user 17min 19s, sys: 10min 47s, total: 28min 6s\n",
            "Wall time: 28min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BbuG1i7sR3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "11a815c3-9225-4452-d5ec-1656b7aaaf11"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "#plt.plot(history['val_accf'], label='Test accuracy on final dataset')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 727,
              "height": 500
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-W1f-_PS4HN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NbaaU-Ysaom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29de5455-0a67-4468-85ea-0095778c6246"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9090909090909092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7MKtnUEshK6"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0JqtuOIwklx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "83b9d5bf-9ee5-40db-bf71-aacf9e74267b"
      },
      "source": [
        "df_test_set_val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>ADE_Drug</th>\n",
              "      <th>ADE_Drug_label</th>\n",
              "      <th>ADE_Drug_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>Recent NSTEMI with decision to restart &lt;DRUG&gt; ...</td>\n",
              "      <td>['bleeding', 'beta blocker']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>Derm was consulted biopsies were taken which w...</td>\n",
              "      <td>['rash', 'Nafcillin']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3642</th>\n",
              "      <td>ID a Pulmonary nodules Pt had history of pulmo...</td>\n",
              "      <td>['periorbital edema', 'ambisome']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189</th>\n",
              "      <td>1213 EEG IMPRESSION Probably normal awake and ...</td>\n",
              "      <td>['Thrombocytopenia', 'steroids']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2807</th>\n",
              "      <td>ROS no CP no SOB no cough Chronic RUQ pain for...</td>\n",
              "      <td>['hypertension', 'risperidone']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>During your hospital stay your surgery to debr...</td>\n",
              "      <td>['rash', 'lamictal']</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>Initial hyperglycemia treated with &lt;DRUG&gt; SSI ...</td>\n",
              "      <td>['hypoglycemia', 'lantus']</td>\n",
              "      <td>0</td>\n",
              "      <td>Positive Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>Authorization for PO vanco could not be obtain...</td>\n",
              "      <td>['leukopenia', 'cipro']</td>\n",
              "      <td>1</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3507</th>\n",
              "      <td>In addition it was felt that at the time &lt;DRUG...</td>\n",
              "      <td>['bleeding', 'plavix']</td>\n",
              "      <td>1</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>Most likely the patient experienced an acute b...</td>\n",
              "      <td>['Transient hypotension', 'Midazolam']</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative Label</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3861 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 corpus  ... ADE_Drug_Rating\n",
              "3538  Recent NSTEMI with decision to restart <DRUG> ...  ...  Negative Label\n",
              "1653  Derm was consulted biopsies were taken which w...  ...  Positive Label\n",
              "3642  ID a Pulmonary nodules Pt had history of pulmo...  ...  Negative Label\n",
              "2189  1213 EEG IMPRESSION Probably normal awake and ...  ...  Negative Label\n",
              "2807  ROS no CP no SOB no cough Chronic RUQ pain for...  ...  Negative Label\n",
              "...                                                 ...  ...             ...\n",
              "1130  During your hospital stay your surgery to debr...  ...  Positive Label\n",
              "1294  Initial hyperglycemia treated with <DRUG> SSI ...  ...  Positive Label\n",
              "860   Authorization for PO vanco could not be obtain...  ...  Negative Label\n",
              "3507  In addition it was felt that at the time <DRUG...  ...  Negative Label\n",
              "3174  Most likely the patient experienced an acute b...  ...  Negative Label\n",
              "\n",
              "[3861 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrXVS9BPsh0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e6ccf5-ddb7-4da9-f786-9a3ecdfaacab"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfYNKaR0Q-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bf9061-5eee-44fe-996c-8948acd8350a"
      },
      "source": [
        "test_data_loaderf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f2dcc939a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3f2QEjuyJ9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87885c1b-ed79-4270-c840-44ff2b9d56de"
      },
      "source": [
        "y_review_textsf, y_predf, y_pred_probsf, y_testf = get_predictions(\n",
        "  model,\n",
        "  test_data_loaderf\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPuuXRD4CNLS"
      },
      "source": [
        "##Result on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84WrfHxvsouT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826d414d-85e9-4d19-d909-461fe8dd72ab"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=['Positive Label','Negative Label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "Positive Label       0.87      0.83      0.85       119\n",
            "Negative Label       0.82      0.86      0.84       104\n",
            "\n",
            "      accuracy                           0.84       223\n",
            "     macro avg       0.84      0.84      0.84       223\n",
            "  weighted avg       0.84      0.84      0.84       223\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DddeA0G8CJZZ"
      },
      "source": [
        "##Results on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlMB4-8_3Jr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc4980d-415a-42e7-cb40-515af0146905"
      },
      "source": [
        "print(classification_report(y_testf, y_predf, target_names=['Positive Label','Negative Label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "Positive Label       0.77      0.83      0.80      2078\n",
            "Negative Label       0.78      0.72      0.75      1783\n",
            "\n",
            "      accuracy                           0.78      3861\n",
            "     macro avg       0.78      0.77      0.77      3861\n",
            "  weighted avg       0.78      0.78      0.78      3861\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}